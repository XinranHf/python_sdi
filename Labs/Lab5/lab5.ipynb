{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 5 - Parallel Markov chains with multiprocessing and dask\n",
    "\n",
    "Students (pair):\n",
    "- [Student 1]([link](https://github.com/username1)) Alexandre MARTEL\n",
    "- [Student 2]([link](https://github.com/username2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references for this lab**:\n",
    "\n",
    "[1] `seaborn`: [official tutorial](https://seaborn.pydata.org/tutorial.html)\n",
    "\n",
    "[2] `multiprocessing`: [documentation](https://docs.python.org/3/library/multiprocessing.html), [doc2](https://he-arc.github.io/livre-python/multiprocessing/index.html)\n",
    "\n",
    "[3] `dask`: [documentation](http://numba.pydata.org/) \n",
    "\n",
    "## <a name=\"content\">Contents</a>\n",
    "- [Exercise 1: seaborn, a useful tool for data visualisation](#ex1)\n",
    "- [Exercise 2: Simulating a discrete-time homogeneous Markov chain](#ex2)\n",
    "- [Bonus: Parallel computing with Dask](#bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex1\">Exercise 1: seaborn, a useful tool for data visualisation</a> [(&#8593;)](#content)\n",
    " \n",
    "The `seaborn` package can significantly enhance data and data analysis visualization. See the [tutorial page](https://seaborn.pydata.org/tutorial.html) for examples of effective predefined graphics. An example aimed at visualizing the empirical distributions of 9 realizations of a bivariate Gaussian random vector is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "rng = np.random.default_rng(50)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "# Rotate the starting point around the cubehelix hue circle\n",
    "for ax, s in zip(axes.flat, np.linspace(0, 3, 10)):\n",
    "\n",
    "    # Create a cubehelix colormap to use with kdeplot\n",
    "    cmap = sns.cubehelix_palette(start=s, light=1, as_cmap=True)\n",
    "\n",
    "    # Generate and plot a random bivariate dataset\n",
    "    x, y = rng.normal(size=(2, 50))\n",
    "    sns.kdeplot(x=x, y=y, cmap=cmap, shade=True, cut=5, ax=ax)\n",
    "    ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comment on the lines of codes related to the `seaborn` library to make their role explicit. More specifically comment on the KDE method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Met le style en noir\n",
    "sns.set(style=\"dark\")\n",
    "\n",
    "rng = np.random.default_rng(50)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "# On crée plusieurs colormaps en faisant varier le paramètre \"start\" du début pour obtenir des couleurs différentes dans chaque graphe\n",
    "for ax, s in zip(axes.flat, np.linspace(0, 3, 10)):\n",
    "\n",
    "    # On créé une palette cubehelix qui est une colormap qui change petit à petit de luminosité tout en restant à peu près uniforme\n",
    "\n",
    "    # - start=s : change le point de départ des couleurs\n",
    "    # - light=1 : permet d’aller jusqu’à une couleur claire\n",
    "    # - as_cmap=True : renvoie une colormap continue\n",
    "    cmap = sns.cubehelix_palette(start=s, light=1, as_cmap=True)\n",
    "\n",
    "    # Génère 50 points aléatoires selon une loi normale pour les coordonnées x et y.\n",
    "    x, y = rng.normal(size=(2, 50))\n",
    "\n",
    "    # Kernel Density Estimation\n",
    "\n",
    "    # Le principe est : Chaque point est remplacé par une gaussienne, elles sont additionnées pour obtenir une fonction de densité lisse qui approxime la proba d’observer des points dans une certaine zone\n",
    "\n",
    "    # - x, y : les inputs\n",
    "    # - cmap= cmap : la colormap définie plus haut\n",
    "    # - shade=True : colore l’intérieur des courbes de densité\n",
    "    # - cut= 5 : prolonge l’estimation de 5 fois l’écart-type du kernel au-delà des points extrêmes\n",
    "    # - ax=ax : indique le graph\n",
    "    sns.kdeplot(x=x, y=y, cmap=cmap, shade=True, cut=5, ax=ax)\n",
    "    ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For one of the realizations, take a look at the documentation of [`sns.jointplot`](https://seaborn.pydata.org/examples/joint_kde.html) to display both the 2-D empirical distribution of the data, and 1D histograms of their distribution along each axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = rng.normal(size=(2, 200))\n",
    "\n",
    "g = sns.jointplot(x=x, y=y,kind=\"kde\",fill=True,cmap=\"mako\")\n",
    "\n",
    "# Ajout d’un titre\n",
    "g.fig.suptitle(\"2-D empirical distribution of the data, and 1D histograms of their distribution\", y=1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex2\">Exercise 2: Simulating a discrete-time homogeneous Markov chain.</a> [(&#8593;)](#content)\n",
    "\n",
    "\n",
    "Let ${(X_n)}_{n\\geq 0}$ be a discrete-time homogeneous Markov chain with values over a finite ensemble $E=\\{x_1,\\dots,x_N\\}$ identified to $\\{1,\\dots,N\\}$. Consider $\\boldsymbol{\\rho} \\in \\Delta_N$, where $\\Delta_N = \\{\\mathbf{x}\\in\\mathbb{R}^N \\mid x_n \\geq 0 \\, \\forall n \\in \\{1,\\dotsc,N\\} \\text{ and } \\sum_n x_n = 1 \\}$ is the unit simplex in $\\mathbb{R}^N$.\n",
    "\n",
    "In the following, we consider the initial state of the chain $X_0$, following the discrete probability distribution:\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(X_0 = k) = \\rho_k, \\qquad k \\in \\{1, \\dots,  N\\}.\n",
    "$$\n",
    "  \n",
    "Let $\\mathbf{A} = [a_{i,j}]_{i,j} \\in \\mathbb{R}^{N \\times N}$ be the transition matrix of the chain, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "    &a_{i,j} = \\mathbb{P}(X_{n+1} = j \\mid X_{n} = i) \\geq 0, \\, \\forall n \\geq 0, \\\\\n",
    "    &(\\forall i \\in \\{1, \\dotsc, N\\}), \\quad \\sum_{j=1}^N a_{i,j} = 1.\n",
    "\\end{align*}\n",
    " \n",
    "The chain is said to be homogeneous in that $\\mathbf{A}$ does not depend from the time index $n$. Let $\\tilde{a}_n$ represent the $n$-th row of $\\mathbf{A}$. \n",
    "\n",
    "The trajectory of the chain can be simulated as follows:\n",
    "\n",
    ">- Draw the discrete random variable $X_0$ with distribution $\\boldsymbol{\\rho}$;\n",
    ">\n",
    ">- For $q = 0$ to $n_{\\text{iter}}-1$\n",
    ">    - Draw the discrete random variable $X_{q+1}$ with distribution $\\tilde{a}_{X_{q}}$;\n",
    ">    \n",
    ">- Return ${(X_q)}_{0 \\leq q \\leq n_{\\text{iter}}}$.\n",
    "\n",
    "\n",
    "<!-- If $X_n = k$, we know that $T$, the life time of the chain in the state $k$ obeys a geometric distribution with parameter $a_{kk}$. We also know that the probability of transition from k to $\\ell\\neq k$ is given by:\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(X_{n+1}=\\ell | X_n=k, \\ell\\neq k) = \\frac{a_{k\\ell}}{1-a_{kk}}.\n",
    "$$\n",
    "\n",
    " ### One possible algorithm to simulate a Markov chain is therefore:\n",
    "\n",
    "    a. generate the initial state $X_0$ according to the discrete law $\\{\\rho_1,\\dots,\\rho_N\\}$.\n",
    "\n",
    "    b. at instant $n$, knowing that $X_n=k$,\n",
    "\n",
    "    i) determine the life time $T$ in state $X_n=k$ by simulating a geometrical variable with parameter $a_{kk}$. As a consequence $X_n = \\dots = X_{n+T} = k$. When $T=0$, we simply still have $X_n=k$.\n",
    "\n",
    "    ii) determine next transition instant $n+T$, and determine the next state by using the probabilities of transition. -->\n",
    "\n",
    "1. Implement the above algorithm in a function `X = markov(rho,A,nmax,rng)` with:\n",
    "     - `rho`: law of the initial state (nonnegative vector of size $N$, summing to 1),\n",
    "     - `A`: transition matrix (of size $N\\times N$) \n",
    "     - `nmax`: number of time steps,\n",
    "     - `rng`: random number generator\n",
    "     - `X`: trajectory of the chain.\n",
    "     \n",
    "In particular, check the input parameters `A` and `rho` make sense by adding appropriate assertions (or raising exceptions).\n",
    "   - Examples : **transition matrix `A` should be square** ; **expected `A` to be a stochastic matrix** ; **verify if the size of `A` and `rho` are consistent** ; **verify if `rho` is in the unit simplex**\n",
    "     - `np.allclose`, `np.isclose` and `np.all` could be useful\n",
    "\n",
    "> Hints:\n",
    "> - the function `np.random.choice` can be useful to draw discrete random variables.\n",
    "> - use `states = np.arange(N)` to create the the labels of the states (from $0$ up to $N-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of execution : \n",
    "![alternatvie text](img/For_CourseM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain(rho, A, nmax, rng):\n",
    "    \"\"\"\n",
    "    Stimulate a discrete-time homogeneous Markov chain\n",
    "\n",
    "    Args:\n",
    "        rho: law of the initial state (nonnegative vector of size $N$, summing to 1)\n",
    "        A: transition matrix (of size $N\\times N$)nmax: number of time steps\n",
    "        rng: random number generator\n",
    "\n",
    "    Returns:\n",
    "        X: trajectory of the chain\n",
    "    \"\"\"\n",
    "\n",
    "    rho = np.asarray(rho)\n",
    "    A = np.asarray(A)\n",
    "\n",
    "    # Check inputs parameters\n",
    "\n",
    "    # Transition matrix A should be square\n",
    "    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(\"transition matrix A should be square\")\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # A is a stochastic matrix\n",
    "    if not np.all(A >= 0):\n",
    "        raise ValueError(\"A must have nonnegative entries\")\n",
    "    if not np.allclose(A.sum(axis=1), 1.0):\n",
    "        raise ValueError(\"Each row of A must sum to 1\")\n",
    "\n",
    "    # Size of rho should be consistent with A\n",
    "    if rho.ndim != 1 or rho.shape[0] != N:\n",
    "        raise ValueError(\"size of rho should be consistent with A\")\n",
    "\n",
    "    # rho should be in the unit simplex\n",
    "    if rho.ndim != 1:\n",
    "        raise ValueError(\"rho must be a 1D array\")\n",
    "    if not np.all(rho >= 0):\n",
    "        raise ValueError(\"rho must have nonnegative entries\")\n",
    "    if not np.isclose(rho.sum(), 1.0):\n",
    "        raise ValueError(\"rho must sum to 1\")\n",
    "\n",
    "    # Simulate the chain\n",
    "    X = np.empty(nmax, dtype=int)\n",
    "    X[0] = rng.choice(N, p=rho)\n",
    "    for n in range(1, nmax):\n",
    "        X[n] = rng.choice(N, p=A[X[n-1], :])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the random number generator to a known state. Make a few simulations using simple transition matrices (*i.e.*, taking any nonnegative matrix $A=(a_{i,j})$ such that its lines sum to 1) and display the trajectory of the chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explore the potential of the [`multiprocessing` package](https://docs.python.org/3/library/multiprocessing.html) to simulate several Markov chains in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hint: the `mutiprocessing.Pool.starmap` or `mutiprocessing.Pool.starmap_async` methods could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Bonus] Generate Markov chains in parallel with the [`dask`](https://docs.dask.org/en/latest/futures.html) library, which offers more general parallelization functionalities (with, for instance, the use of [`Futures`](https://docs.dask.org/en/stable/futures.html), see tutorial [here](https://tutorial.dask.org/05_futures.html)). A useful example is provided [here](https://stackoverflow.com/questions/41471248/how-to-efficiently-submit-tasks-with-large-arguments-in-dask-distributed). Note that `dask` is much more versatile and powerful than `multiprocessing`, and can be useful to scale algorithms over multiple cores and/or computing nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"bonus\">Bonus: Parallel computing with Dask</a> [(&#8593;)](#content)\n",
    "\n",
    "1. Take a look at the [`dask.array` documentation](https://docs.dask.org/en/stable/array-best-practices.html) and the associate [tutorial](https://tutorial.dask.org/02_array.html). Apply some of the functions introduced herein and in the [documentation](https://docs.dask.org/en/stable/array-best-practices.html) to parallelize the computation of the total variation investigated during session 2. Note that you can combine `dask` and `numba` to obtain an overall more efficient implementation. Note that timing can be worse than Numpy (`dask.array` is more specifically interesting when the data do no fit in memory).\n",
    "\n",
    "2. Take a look at the [`dask.delayed` tutorial](https://tutorial.dask.org/03_dask.delayed.html), and go through some of the examples provided. [Best practices with the `dask.delayed` interface](https://docs.dask.org/en/stable/delayed-best-practices.html) are summarized in the documentation.\n",
    "\n",
    "> **Remark**: an alternative to Dask: the [Ray](https://docs.ray.io/en/latest/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai seulement fait des fonctions de test qui ne sont pas vraiment utilisables, mais qui servent simplement à tester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction qui calcule la somme des distances euclidiennes entre toutes les paires de lignes d'une matrice X. Le but est de faire des calcul assez lourd dans le but de comparer les performances de différentes implémentations. (avec Dask et numba)\n",
    "\n",
    "# VERSION PYTHON VANILLA\n",
    "\n",
    "def pairwise_dist_py(X):\n",
    "    m, n = X.shape\n",
    "    dist_sum = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(i+1, m):\n",
    "            s = 0.0\n",
    "            for k in range(n):\n",
    "                diff = X[i, k] - X[j, k]\n",
    "                s += diff * diff\n",
    "            dist_sum += s**0.5\n",
    "    return dist_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Fonction qui calcule la somme des distances euclidiennes entre toutes les paires de lignes d'une matrice X. Le but est de faire des calcul assez lourd dans le but de comparer les performances de différentes implémentations. (avec Dask et numba)\n",
    "\n",
    "#VERSION DASK (on doit changer les méthode de calcul pour faire du calcul par blocs et éviter les boucles/ boucles imbriquées dans Dask, donc une méthode différente peut faire aussi varier les temps de calcul, mais c'est aussi ce que cela sous-entend de se servir de Dask, donc je considère que c'est ok)\n",
    "\n",
    "def pairwise_dist_dask(X):\n",
    "\n",
    "    norms = (X**2).sum(axis=1).reshape((-1, 1))  # ||x_i||^2\n",
    "    gram = X @ X.T    # <x_i, x_j> = prod scalaire\n",
    "\n",
    "    D2 = norms + norms.T - 2 * gram # ||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2<x_i, x_j>\n",
    "    D2 = da.maximum(D2, 0.0)  # éviter les possibles petites valeurs négatives dues aux arrondis\n",
    "\n",
    "    D = da.sqrt(D2) #distance euclidienne\n",
    "\n",
    "    # On ne garde que la partie triangulaire supérieure (i < j) car la matrice est symétrique et on ne veut pas compter deux fois chaque distance\n",
    "    tri = da.triu(D, k=1)\n",
    "\n",
    "    return tri.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "# Fonction qui calcule la somme des distances euclidiennes entre toutes les paires de lignes d'une matrice X. Le but est de faire des calcul assez lourd dans le but de comparer les performances de différentes implémentations. (avec Dask et numba)\n",
    "\n",
    "#VERSION NUMBA (Les paramètres parallel=True et fastmath=True permettent d'optimiser les calculs en utilisant du parallélisme et des optimisations mathématiques). Le prange est une version parallèle de range.\n",
    "\n",
    "#Le parametre fastmath=True permet de faire des calcul et des arrondis assez aggressifs pour gagner en performance, mais cela peut aussi faire perdre en précision.\n",
    "\n",
    "#Le paramètre parallel=True permet d'utiliser plusieurs coeurs du CPU pour faire les calculs de la boucle qui a un prange dans différents threads en parallèle, au lieu de faire les itérations séquentiellement. La boucle d'après doit être en range et non prange car elle se déroule elle même au sein d'un thread et on peut risquer une surpopulation de threah sinon.\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def pairwise_dist_numba(X):\n",
    "    m, n = X.shape\n",
    "    dist_sum = 0.0\n",
    "    for i in numba.prange(m):\n",
    "        for j in range(i+1, m):\n",
    "            s = 0.0\n",
    "            for k in range(n):\n",
    "                diff = X[i, k] - X[j, k]\n",
    "                s += diff * diff\n",
    "            dist_sum += s**0.5\n",
    "    return dist_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(2000, 30) # matrice de test, lourde de preférence\n",
    "dX = da.from_array(X, chunks=(500, 30))\n",
    "\n",
    "# test en Python pur\n",
    "t0 = time.time()\n",
    "res_py = pairwise_dist_py(X)\n",
    "print(\"Python pur  :\", res_py, \"temps =\", time.time() - t0, \"s\")\n",
    "\n",
    "# test pour Dask\n",
    "t0 = time.time()\n",
    "res_dask = pairwise_dist_dask(dX).compute()\n",
    "print(\"Dask :\", res_dask, \"temps =\", time.time() - t0, \"s\")\n",
    "\n",
    "# test pour Numba\n",
    "t0 = time.time()\n",
    "res_numba = pairwise_dist_numba(X)\n",
    "print(\"Numba :\", res_numba, \"temps =\", time.time() - t0, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "X = np.random.rand(10000, 10000)\n",
    "\n",
    "def gradient2D_dask_bis(X):\n",
    "    dx = da.diff(X, axis=1, append=X[:, -1:])\n",
    "    dy = da.diff(X, axis=0, append=X[-1:, :])\n",
    "    grad = da.stack([dx, dy], axis=2)\n",
    "    return da.sqrt((grad**2).sum(axis=2)).sum()\n",
    "\n",
    "@dask.delayed\n",
    "def process(X):\n",
    "    data = gradient2D_dask_bis(X)\n",
    "    return data\n",
    "\n",
    "results = [process(X) for _ in range(1)]\n",
    "\n",
    "t0 = time.time()\n",
    "res_dask = dask.compute(results)\n",
    "t_dask = time.time() - t0\n",
    "print(\"dask.delayed -\" , \"temps =\", t_dask, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les différentes méthodes améliorent beaucoup le temps d'execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
